{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PHuEtm7j3TB",
        "outputId": "ce513d82-688c-4602-ede8-a316a723c346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m235.5/244.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx\n",
        "!pip install PyPDF2\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "import PyPDF2\n",
        "from transformers import pipeline\n",
        "from google.colab import files\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "file_name = next(iter(uploaded))\n",
        "\n",
        "# Initialize summarizer pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Function to read .docx\n",
        "def read_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    full_text = []\n",
        "    for para in doc.paragraphs:\n",
        "        full_text.append(para.text)\n",
        "    return '\\n'.join(full_text)\n",
        "\n",
        "# Function to read .pdf\n",
        "def read_pdf(file_path):\n",
        "    reader = PyPDF2.PdfReader(file_path)\n",
        "    full_text = ''\n",
        "    for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            full_text += page_text + '\\n'\n",
        "    return full_text\n",
        "\n",
        "# Function to read .csv\n",
        "def read_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df.to_string(index=False)\n",
        "\n",
        "# Function to chunk long text\n",
        "def chunk_text(text, max_chunk_words=500):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i+max_chunk_words]) for i in range(0, len(words), max_chunk_words)]\n",
        "\n",
        "# Detect file extension\n",
        "_, ext = os.path.splitext(file_name)\n",
        "\n",
        "# Extract text\n",
        "if ext == '.docx':\n",
        "    text = read_docx(file_name)\n",
        "elif ext == '.pdf':\n",
        "    text = read_pdf(file_name)\n",
        "elif ext == '.csv':\n",
        "    text = read_csv(file_name)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported file format: {ext}\")\n",
        "\n",
        "# Chunk the text\n",
        "chunks = chunk_text(text, max_chunk_words=500)\n",
        "\n",
        "# Summarize each chunk\n",
        "print(\" Summarizing document in chunks...\\n\")\n",
        "summaries = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\" Chunk {i+1}/{len(chunks)}...\")\n",
        "    summary = summarizer(chunk, max_length=200, min_length=60, do_sample=False)[0]['summary_text']\n",
        "    summaries.append(summary)\n",
        "\n",
        "# Combine summaries\n",
        "final_summary = '\\n\\n'.join(summaries)\n",
        "\n",
        "# Print result\n",
        "print(\"\\n Final Summary:\\n\")\n",
        "print(final_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "trqMlVqAj6Dd",
        "outputId": "e1bc647c-3fec-43d4-caa8-b6927f73f075"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-11cf763f-d8f9-46f0-b04e-2ef7d49266cd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-11cf763f-d8f9-46f0-b04e-2ef7d49266cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1748193927774.pdf to 1748193927774 (1).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Summarizing document in chunks...\n",
            "\n",
            "ğŸ§© Chunk 1/26...\n",
            "ğŸ§© Chunk 2/26...\n",
            "ğŸ§© Chunk 3/26...\n",
            "ğŸ§© Chunk 4/26...\n",
            "ğŸ§© Chunk 5/26...\n",
            "ğŸ§© Chunk 6/26...\n",
            "ğŸ§© Chunk 7/26...\n",
            "ğŸ§© Chunk 8/26...\n",
            "ğŸ§© Chunk 9/26...\n",
            "ğŸ§© Chunk 10/26...\n",
            "ğŸ§© Chunk 11/26...\n",
            "ğŸ§© Chunk 12/26...\n",
            "ğŸ§© Chunk 13/26...\n",
            "ğŸ§© Chunk 14/26...\n",
            "ğŸ§© Chunk 15/26...\n",
            "ğŸ§© Chunk 16/26...\n",
            "ğŸ§© Chunk 17/26...\n",
            "ğŸ§© Chunk 18/26...\n",
            "ğŸ§© Chunk 19/26...\n",
            "ğŸ§© Chunk 20/26...\n",
            "ğŸ§© Chunk 21/26...\n",
            "ğŸ§© Chunk 22/26...\n",
            "ğŸ§© Chunk 23/26...\n",
            "ğŸ§© Chunk 24/26...\n",
            "ğŸ§© Chunk 25/26...\n",
            "ğŸ§© Chunk 26/26...\n",
            "\n",
            " Final Summary:\n",
            "\n",
            "When you chat with the Gemini chatbot,1 you basically write prompts. This whitepaper focuses on writing prompts for the Gemini model within Vertex AI or by using the API. We will look into the various prompting techniques to help you getting started and share tips and best practices to become a prompting expert.\n",
            "\n",
            " Prompt engineering is the process of designing high-quality prompts that guide LLMs to produce accurate outputs. This process involves tinkering to find the best prompt, optimizing prompt length, and evaluating a promptâ€™s writing style and structure in relation to the task. When prompt engineering, you will start by choosing a model.\n",
            "\n",
            "The Gemini temperature control can be understood in a similar way to the softmax function used in machine learning. A low temperature setting mirrors a low softmax temperature (T), emphasizing a single, preferred temperature with high certainty. Temperatures close to the max tend to create more random output. As temperature gets higher and higher, all tokens become equally likely to be the next predicted token. The higher top-K, the more creative and varied the modelâ€™s output.\n",
            "\n",
            "At extreme settings of one sampling configuration value, that one sampling setting either cancels out other configuration settings or becomes irrelevant. As a general starting point, a temperature of .2, top-P of .95, and top-K of 30 will give you relatively coherent results that can be creative but not excessively so. If your task always has a single correct answer (e.g., answering a math problem), start with a temperature. of 0.\n",
            "\n",
            "When creating prompts for AI models, it is helpful to provide examples. Examples are especially useful when you want to steer the model to a certain output structure or pattern. When zero-shot doesnâ€™t work, you can provide demonstrations or examples in the prompt, which leads to â€œone-shotâ€ and â€œfew- shotâ€ prompting.\n",
            "\n",
            "System, contextual and role prompting are all techniques used to guide how LLMs generate text. System prompting sets the overall context and purpose for the language model. Contextual prompting provides specific details or background information relevant to the current conversation or task. Role prompting assigns a specific character or identity for the model to adopt.\n",
            "\n",
            "System prompts can be useful for generating output that meets specific requirements. The name â€˜system promptâ€™ actually stands for â€˜providing an additional task to the systemâ€™ For example, you could use a system prompt to generate a code snippet that is compatible with a specific programming language. Role prompting is a technique in prompt engineering that involves assigning a specific role to the gen AI model.\n",
            "\n",
            "Artificial intelligence can act as a travel guide, suggesting places to visit. Defining a role perspective for an AI model gives it a blueprint of the tone, style, and focused expertise youâ€™re looking for to improve the quality, relevance, and effectiveness of your output. Here are some styles you can choose from which I find effective.\n",
            "\n",
            "Suggest 3 topics to write an article about with a few lines of description of what this article should contain. The Evolution of Arcade Cabinet Design:** This article would explore the evolution of arcade cabinet designs, from the early wood and metal cabinets of the 1970s to the sleek, neon-lit designs of the 1980s and beyond. Blast From The Past: Iconic Arcade Games of The 80â€™s:** - this article would feature a glimpse into the history, impact, and legacy of some of the most iconic and influential arcade games of the 80s.\n",
            "\n",
            "Abandoned Military Base: A sprawling, post-apocalyptic military complex crawling with mutated soldiers and rogue robots. Cyberpunk City: A neon-lit, futuristic urban environment with towering skyscrapers and dense alleyways. Alien Spaceship: A vast alien vessel stranded on Earth, with eerie corridors, zero-gravity sections, and extraterrestrial creatures to encounter. Zombie-Infested Town: A desolate town overrun by hordes of aggressive zombies, featuring intense close-quarters combat and puzzle-solving to find safe passage. Underwater Research Facility: A deep-sea laboratory flooded with water, filled with mutated aquatic creatures, and requiring stealth and underwater exploration skills to survive.\n",
            "\n",
            "Chain of Thought (CoT) 9 prompting is a technique for improving the reasoning capabilities of LLMs by generating intermediate reasoning steps. This helps the LLM generate more accurate answers. CoT has a lot of advantages. Itâ€™s low-effort while being very effective and works well with off-the-shelf LLMs (so no need to finetune)\n",
            "\n",
            "Chain of thought prompting can be very powerful when combined with a single-shot or few-shot. Chain of thought can be useful for various use-cases, such as code generation. In the best practices section of this chapter, we will learn some best practices specific to Chain of Thought prompting.\n",
            "\n",
            "CoT uses a simple â€˜greedy decodingâ€™ strategy, limiting its effectiveness. Self-consistency11 combines sampling and majority voting to generate diverse reasoning paths and select the most consistent answer. It improves the accuracy and coherence of responses generated by LLMs. A zero-shot chain of thought prompt will be sent to the LLM multiple times.\n",
            "\n",
            "The sender of the email claims to be a hacker named Harry. There is no way to verify the senderâ€™s identity. It is possible that the sender is a legitimate hacker who is trying to help the recipient, but it is also possible that they are a malicious actor. The bug in the contact form could allow an attacker to execute arbitrary JavaScript code.\n",
            "\n",
            "Self-consistency prompting can be used to improve the accuracy of an LLMâ€™s response by considering multiple perspectives and selecting the most consistent answer. Tree of Thoughts (ToT) generalizes the concept of CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously. ReAct prompting works by combining reasoning and acting into a thought-action loop. To see this in action, you need to write some code.\n",
            "\n",
            "ReAct makes a chain of five searches. In fact, the LLM is scraping Google search results to figure out the band names. Then, it lists the results as observations and chains the thought for the next search. For example, you can use automatic prompt engineering that will help with training a chatbot for a merchandise t-shirt webshop.\n",
            "\n",
            "Gemini is a chatbot that can be used to write code. It can also be a developer and help you with writing code in any programming language of choice. Letâ€™s go to the Vertex AI Studio and test these prompts to look at some coding examples. Prompt Engineering September 202442Code prompting Gemini focuses primarily on text-based prompts, which also includes writing prompts for returning code.\n",
            "\n",
            "Rename_ files.sh is a Bash script that renames files in a test folder. It asks the user to enter a folder name using the `echo` and `read` commands. It then renames the file using the â€œmv â€œ$fileâ€ â€œ $new_file_nameâ€ command, which moves the file to the new file name. After renaming all the files, it displays a success message (â€œFiles renamed successfullyâ€)\n",
            "\n",
            "Python would be a better language for a (web) application than Bash. LLMs can help with translating code from one language to another. See the example in Table 18: Prompt Engineering September 202447Name 1_python_rename_ files Goal Write a prompt to translate Bash code to Python Model gemini-pro Temperature 0.1 Token Limit 1024 Top-K N/A Top-P 1 Prompt Translate the below Bash code into a Python snippet.\n",
            "\n",
            "The code calls the `toUpperCase` function to convert ` prefix` to uppercase, but that function is not defined. To fix this issue, you can use the `upper()` method of the string class. The code doesnâ€™t handle errors that might occur during the renaming process. It would be better to wrap the `shutilmove` call in a `try...except` block.\n",
            "\n",
            "Language Studio in Vertex AI is a perfect place to play around with your prompts. Use the following best practices to become a pro in prompt engineering. Prompts should be concise, clear, and easy to understand for both you and the model. The most important best practice is to provide (one shot / few shot) examples within a prompt.\n",
            "\n",
            "Be specific about the desired output. Constraints are still valuable but in certain situations. To control the length of a generated LLM response, you can either set a max token limit in the configuration or explicitly request a specific length in your prompt. To reuse prompts and make it more dynamic use variables.\n",
            "\n",
            " Variables can save you time and effort by allowing you to avoid repeating yourself. Experiment with input formats and writing styles. Mix up the possible response classes in the few shot examples. For non- creative tasks like extracting, selecting, parsing, ordering, ranking, or categorizing data try having your output returned in a structured format like JSON or XML.\n",
            "\n",
            "Document your prompt attempts in full detail so you can learn over time what went well and what did not. For CoT prompting, put the answer after the reasoning is required because the generation of the reasoning changes the tokens that the model gets when it predicts the final answer. Save your prompts (using the same name and version as listed in your documentation) and track the hyperlink.\n",
            "\n",
            "Once you feel the prompt is close to perfect, take it to your project codebase. Save prompts in a separate file from code, so itâ€™s easier to maintain. ideally your prompts are part of an operationalized system. As a prompt engineer you should rely on automated tests and evaluation procedures.\n",
            "\n",
            "Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Available at: https://arxiv.org/pdf/2305.10601.pdf . Yao, S., et al., 2023, ReAct: Synergizing Reasoning and Acting in Language models. Google Cloud Platform, 2023,. Advance Prompting: Chain of Thought and React.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GPAhVs-dkDmL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}